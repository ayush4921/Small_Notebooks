{"cells":[{"cell_type":"code","execution_count":29,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"mw6Vq6aHc0si"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.3.1\n"]},{"output_type":"execute_result","data":{"text/plain":["      count          mean           std  min      25%      50%       75%  \\\n","num  3362.0   2113.099941   1219.826583  1.0  1054.25   2123.5   3170.75   \n","sop  3362.0  18959.858418  11845.640866  3.0  8434.00  18547.0  29177.50   \n","\n","         max  \n","num   4202.0  \n","sop  39989.0  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>num</th>\n      <td>3362.0</td>\n      <td>2113.099941</td>\n      <td>1219.826583</td>\n      <td>1.0</td>\n      <td>1054.25</td>\n      <td>2123.5</td>\n      <td>3170.75</td>\n      <td>4202.0</td>\n    </tr>\n    <tr>\n      <th>sop</th>\n      <td>3362.0</td>\n      <td>18959.858418</td>\n      <td>11845.640866</td>\n      <td>3.0</td>\n      <td>8434.00</td>\n      <td>18547.0</td>\n      <td>29177.50</td>\n      <td>39989.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":29}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","\n","\n","# Make numpy printouts easier to read.\n","np.set_printoptions(precision=3, suppress=True)\n","import tensorflow as tf\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers.experimental import preprocessing\n","print(tf.__version__)\n","prime1=[]\n","# import pandas as pd \n","def SieveOfEratosthenes(n): \n","       \n","    # Create a boolean array \"prime[0..n]\" and  \n","    # initialize all entries it as true. A value  \n","    # in prime[i] will finally be false if i is \n","    # Not a prime, else true. \n","    prime = [True for i in range(n+1)] \n","      \n","    p = 2\n","    while(p * p <= n): \n","           \n","        # If prime[p] is not changed, then it is  \n","       # a prime \n","        if (prime[p] == True): \n","               \n","            # Update all multiples of p \n","            for i in range(p * p, n + 1, p): \n","                prime[i] = False\n","        p += 1\n","    c = 0\n","  \n","    # Print all prime numbers \n","    for p in range(2, n): \n","        if prime[p]: \n","            prime1.append(p)\n","    return prime1\n","  \n","prime=SieveOfEratosthenes(40000)\n","\n","num=[]\n","newlist=[]\n","for i in range(len(prime)):\n","    s=prime[:i]\n","    newlist.append(sum(s))\n","    num.append(i)\n"," \n"," \n","  \n","# Calling DataFrame constructor after zipping \n","# both lists, with columns specified \n","dataset = pd.DataFrame(list(zip(num, prime1)), \n","               columns =['num', 'sop']) \n","\n","train_dataset = dataset.sample(frac=0.8, random_state=0)\n","test_dataset = dataset.drop(train_dataset.index)\n","train_dataset.describe().transpose()\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"background_save":true},"id":"ZoX6rpM3dH61"},"outputs":[],"source":["train_features = train_dataset.copy()\n","test_features = test_dataset.copy()\n","\n","train_labels = train_features.pop('sop')\n","test_labels = test_features.pop('sop')"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"background_save":true},"id":"4AEooxNxdOUR"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"'NoneType' object is not subscriptable","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-14-87a6a0785717>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnormalizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnormalizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_preprocessing_layer.py\u001b[0m in \u001b[0;36madapt\u001b[1;34m(self, data, reset_state)\u001b[0m\n\u001b[0;32m    213\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m     \u001b[0mupdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_combiner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccumulator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_state_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\preprocessing\\normalization.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(self, accumulator)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[1;34m\"\"\"Convert an accumulator into a dict of output values.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m     return {\n\u001b[1;32m--> 284\u001b[1;33m         \u001b[0m_COUNT_NAME\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOUNT_IDX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m         \u001b[0m_MEAN_NAME\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0m_VARIANCE_NAME\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"]}],"source":["normalizer = preprocessing.Normalization()\n","normalizer.adapt(np.array(train_features))\n","print(normalizer.mean.numpy())\n"]},{"cell_type":"code","execution_count":100,"metadata":{"colab":{"background_save":true},"id":"LBiif67gdbha"},"outputs":[],"source":["horsepower = np.array(train_features['num'])\n","\n","horsepower_normalizer = preprocessing.Normalization(input_shape=[1,])\n","horsepower_normalizer.adapt(horsepower)"]},{"cell_type":"code","execution_count":101,"metadata":{"colab":{"background_save":true},"id":"ZeAQbu2_dlsR"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_18\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nnormalization_19 (Normalizat (None, 1)                 3         \n_________________________________________________________________\ndense_36 (Dense)             (None, 1)                 2         \n=================================================================\nTotal params: 5\nTrainable params: 2\nNon-trainable params: 3\n_________________________________________________________________\n"]}],"source":["horsepower_model = tf.keras.Sequential([\n","    horsepower_normalizer,\n","    layers.Dense(units=1)\n","])\n","\n","horsepower_model.summary()"]},{"cell_type":"code","execution_count":102,"metadata":{"colab":{"background_save":true},"id":"1SttzQUadpM6"},"outputs":[],"source":["horsepower_model.compile(\n","    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n","    loss='mean_absolute_error')"]},{"cell_type":"code","execution_count":103,"metadata":{"colab":{"background_save":true},"id":"uJxOjTnydy6L"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wall time: 4.52 s\n"]}],"source":["%%time\n","history = horsepower_model.fit(\n","    train_features['num'], train_labels,\n","    epochs=100,\n","    # suppress logging\n","    verbose=0,\n","    # Calculate validation results on 20% of the training data\n","    validation_split = 0.2)"]},{"cell_type":"code","execution_count":104,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1S94De6ecyRcp8y65Vi2P0K6GkhRTP1YB"},"id":"mmYUnVREfve1","outputId":"5bfa5fc8-f855-4820-acfd-3544673f94bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_19\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","normalization_19 (Normalizat (None, 1)                 3         \n","_________________________________________________________________\n","dense_37 (Dense)             (None, 100)               200       \n","_________________________________________________________________\n","dense_38 (Dense)             (None, 100)               10100     \n","_________________________________________________________________\n","dense_39 (Dense)             (None, 1)                 101       \n","=================================================================\n","Total params: 10,404\n","Trainable params: 10,401\n","Non-trainable params: 3\n","_________________________________________________________________\n","Epoch 1/250\n","11/11 [==============================] - 0s 11ms/step - loss: 1843.4464 - val_loss: 1825.8026\n","Epoch 2/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1843.3993 - val_loss: 1825.7574\n","Epoch 3/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1843.3542 - val_loss: 1825.7141\n","Epoch 4/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1843.3101 - val_loss: 1825.6711\n","Epoch 5/250\n","11/11 [==============================] - 0s 7ms/step - loss: 1843.2657 - val_loss: 1825.6277\n","Epoch 6/250\n","11/11 [==============================] - 0s 7ms/step - loss: 1843.2208 - val_loss: 1825.5825\n","Epoch 7/250\n","11/11 [==============================] - 0s 8ms/step - loss: 1843.1735 - val_loss: 1825.5352\n","Epoch 8/250\n","11/11 [==============================] - 0s 7ms/step - loss: 1843.1227 - val_loss: 1825.4824\n","Epoch 9/250\n","11/11 [==============================] - 0s 6ms/step - loss: 1843.0665 - val_loss: 1825.4243\n","Epoch 10/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1843.0070 - val_loss: 1825.3647\n","Epoch 11/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1842.9446 - val_loss: 1825.3024\n","Epoch 12/250\n","11/11 [==============================] - 0s 7ms/step - loss: 1842.8798 - val_loss: 1825.2360\n","Epoch 13/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1842.8105 - val_loss: 1825.1644\n","Epoch 14/250\n","11/11 [==============================] - 0s 6ms/step - loss: 1842.7351 - val_loss: 1825.0878\n","Epoch 15/250\n","11/11 [==============================] - 0s 7ms/step - loss: 1842.6544 - val_loss: 1825.0083\n","Epoch 16/250\n","11/11 [==============================] - 0s 8ms/step - loss: 1842.5724 - val_loss: 1824.9243\n","Epoch 17/250\n","11/11 [==============================] - 0s 7ms/step - loss: 1842.4852 - val_loss: 1824.8368\n","Epoch 18/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1842.3937 - val_loss: 1824.7418\n","Epoch 19/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1842.2938 - val_loss: 1824.6406\n","Epoch 20/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1842.1896 - val_loss: 1824.5337\n","Epoch 21/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1842.0796 - val_loss: 1824.4222\n","Epoch 22/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1841.9637 - val_loss: 1824.3051\n","Epoch 23/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1841.8430 - val_loss: 1824.1804\n","Epoch 24/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1841.7145 - val_loss: 1824.0508\n","Epoch 25/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1841.5825 - val_loss: 1823.9144\n","Epoch 26/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1841.4425 - val_loss: 1823.7683\n","Epoch 27/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1841.2925 - val_loss: 1823.6160\n","Epoch 28/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1841.1375 - val_loss: 1823.4562\n","Epoch 29/250\n","11/11 [==============================] - 0s 6ms/step - loss: 1840.9747 - val_loss: 1823.2883\n","Epoch 30/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1840.8031 - val_loss: 1823.1136\n","Epoch 31/250\n","11/11 [==============================] - 0s 7ms/step - loss: 1840.6262 - val_loss: 1822.9292\n","Epoch 32/250\n","11/11 [==============================] - 0s 6ms/step - loss: 1840.4395 - val_loss: 1822.7383\n","Epoch 33/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1840.2469 - val_loss: 1822.5398\n","Epoch 34/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1840.0460 - val_loss: 1822.3328\n","Epoch 35/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1839.8374 - val_loss: 1822.1161\n","Epoch 36/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1839.6199 - val_loss: 1821.8894\n","Epoch 37/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1839.3929 - val_loss: 1821.6527\n","Epoch 38/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1839.1536 - val_loss: 1821.4161\n","Epoch 39/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1838.9081 - val_loss: 1821.1692\n","Epoch 40/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1838.6505 - val_loss: 1820.9125\n","Epoch 41/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1838.3835 - val_loss: 1820.6445\n","Epoch 42/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1838.1046 - val_loss: 1820.3661\n","Epoch 43/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1837.8164 - val_loss: 1820.0762\n","Epoch 44/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1837.5138 - val_loss: 1819.7762\n","Epoch 45/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1837.2013 - val_loss: 1819.4657\n","Epoch 46/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1836.8774 - val_loss: 1819.1431\n","Epoch 47/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1836.5442 - val_loss: 1818.8082\n","Epoch 48/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1836.2017 - val_loss: 1818.4618\n","Epoch 49/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1835.8458 - val_loss: 1818.1044\n","Epoch 50/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1835.4796 - val_loss: 1817.7344\n","Epoch 51/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1835.0988 - val_loss: 1817.3567\n","Epoch 52/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1834.7172 - val_loss: 1816.9620\n","Epoch 53/250\n","11/11 [==============================] - 0s 6ms/step - loss: 1834.3164 - val_loss: 1816.5571\n","Epoch 54/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1833.9032 - val_loss: 1816.1407\n","Epoch 55/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1833.4785 - val_loss: 1815.7036\n","Epoch 56/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1833.0320 - val_loss: 1815.2535\n","Epoch 57/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1832.5757 - val_loss: 1814.7887\n","Epoch 58/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1832.1102 - val_loss: 1814.3109\n","Epoch 59/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1831.6245 - val_loss: 1813.8185\n","Epoch 60/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1831.1321 - val_loss: 1813.3055\n","Epoch 61/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1830.6211 - val_loss: 1812.7841\n","Epoch 62/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1830.1023 - val_loss: 1812.2473\n","Epoch 63/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1829.5653 - val_loss: 1811.6993\n","Epoch 64/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1829.0245 - val_loss: 1811.1285\n","Epoch 65/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1828.4595 - val_loss: 1810.5490\n","Epoch 66/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1827.8898 - val_loss: 1809.9563\n","Epoch 67/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1827.3040 - val_loss: 1809.3507\n","Epoch 68/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1826.7058 - val_loss: 1808.7297\n","Epoch 69/250\n","11/11 [==============================] - 0s 6ms/step - loss: 1826.0883 - val_loss: 1808.0980\n","Epoch 70/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1825.4622 - val_loss: 1807.4471\n","Epoch 71/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1824.8208 - val_loss: 1806.7777\n","Epoch 72/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1824.1613 - val_loss: 1806.0925\n","Epoch 73/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1823.4833 - val_loss: 1805.3947\n","Epoch 74/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1822.7987 - val_loss: 1804.6793\n","Epoch 75/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1822.0988 - val_loss: 1803.9524\n","Epoch 76/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1821.3844 - val_loss: 1803.2128\n","Epoch 77/250\n","11/11 [==============================] - 0s 2ms/step - loss: 1820.6694 - val_loss: 1802.4426\n","Epoch 78/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1819.9130 - val_loss: 1801.6704\n","Epoch 79/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1819.1512 - val_loss: 1800.8860\n","Epoch 80/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1818.3829 - val_loss: 1800.0992\n","Epoch 81/250\n","11/11 [==============================] - 0s 2ms/step - loss: 1817.5879 - val_loss: 1799.2980\n","Epoch 82/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1816.7816 - val_loss: 1798.4764\n","Epoch 83/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1815.9473 - val_loss: 1797.6466\n","Epoch 84/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1815.1141 - val_loss: 1796.7992\n","Epoch 85/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1814.2700 - val_loss: 1795.9554\n","Epoch 86/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1813.4070 - val_loss: 1795.0992\n","Epoch 87/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1812.5188 - val_loss: 1794.2340\n","Epoch 88/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1811.6290 - val_loss: 1793.3464\n","Epoch 89/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1810.7179 - val_loss: 1792.4539\n","Epoch 90/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1809.7954 - val_loss: 1791.5476\n","Epoch 91/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1808.8673 - val_loss: 1790.6165\n","Epoch 92/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1807.9070 - val_loss: 1789.6737\n","Epoch 93/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1806.9426 - val_loss: 1788.7134\n","Epoch 94/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1805.9698 - val_loss: 1787.7343\n","Epoch 95/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1804.9719 - val_loss: 1786.7443\n","Epoch 96/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1803.9652 - val_loss: 1785.7329\n","Epoch 97/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1802.9355 - val_loss: 1784.7266\n","Epoch 98/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1801.9028 - val_loss: 1783.7052\n","Epoch 99/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1800.8235 - val_loss: 1782.6903\n","Epoch 100/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1799.7653 - val_loss: 1781.6495\n","Epoch 101/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1798.6710 - val_loss: 1780.6028\n","Epoch 102/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1797.5883 - val_loss: 1779.5188\n","Epoch 103/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1796.4684 - val_loss: 1778.4301\n","Epoch 104/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1795.3419 - val_loss: 1777.3274\n","Epoch 105/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1794.2000 - val_loss: 1776.2280\n","Epoch 106/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1793.0522 - val_loss: 1775.1204\n","Epoch 107/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1791.8798 - val_loss: 1774.0062\n","Epoch 108/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1790.6978 - val_loss: 1772.8789\n","Epoch 109/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1789.4996 - val_loss: 1771.7379\n","Epoch 110/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1788.2841 - val_loss: 1770.5829\n","Epoch 111/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1787.0742 - val_loss: 1769.4011\n","Epoch 112/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1785.8339 - val_loss: 1768.2109\n","Epoch 113/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1784.5856 - val_loss: 1767.0103\n","Epoch 114/250\n","11/11 [==============================] - 0s 2ms/step - loss: 1783.3458 - val_loss: 1765.7814\n","Epoch 115/250\n","11/11 [==============================] - 0s 2ms/step - loss: 1782.0610 - val_loss: 1764.5515\n","Epoch 116/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1780.7878 - val_loss: 1763.2909\n","Epoch 117/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1779.4762 - val_loss: 1762.0171\n","Epoch 118/250\n","11/11 [==============================] - 0s 2ms/step - loss: 1778.1434 - val_loss: 1760.7343\n","Epoch 119/250\n","11/11 [==============================] - 0s 2ms/step - loss: 1776.8107 - val_loss: 1759.4253\n","Epoch 120/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1775.4446 - val_loss: 1758.0991\n","Epoch 121/250\n","11/11 [==============================] - 0s 2ms/step - loss: 1774.0648 - val_loss: 1756.7645\n","Epoch 122/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1772.6801 - val_loss: 1755.4132\n","Epoch 123/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1771.2949 - val_loss: 1754.0571\n","Epoch 124/250\n","11/11 [==============================] - 0s 6ms/step - loss: 1769.8687 - val_loss: 1752.7129\n","Epoch 125/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1768.4536 - val_loss: 1751.3495\n","Epoch 126/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1767.0135 - val_loss: 1749.9746\n","Epoch 127/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1765.5558 - val_loss: 1748.5985\n","Epoch 128/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1764.1160 - val_loss: 1747.1925\n","Epoch 129/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1762.6414 - val_loss: 1745.7711\n","Epoch 130/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1761.1583 - val_loss: 1744.3345\n","Epoch 131/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1759.6685 - val_loss: 1742.8810\n","Epoch 132/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1758.1525 - val_loss: 1741.4199\n","Epoch 133/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1756.6385 - val_loss: 1739.9265\n","Epoch 134/250\n","11/11 [==============================] - 0s 2ms/step - loss: 1755.0809 - val_loss: 1738.4281\n","Epoch 135/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1753.5282 - val_loss: 1736.8986\n","Epoch 136/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1751.9396 - val_loss: 1735.3566\n","Epoch 137/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1750.3218 - val_loss: 1733.8196\n","Epoch 138/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1748.7446 - val_loss: 1732.2401\n","Epoch 139/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1747.1335 - val_loss: 1730.6438\n","Epoch 140/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1745.5070 - val_loss: 1729.0405\n","Epoch 141/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1743.8624 - val_loss: 1727.4282\n","Epoch 142/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1742.2133 - val_loss: 1725.8328\n","Epoch 143/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1740.5199 - val_loss: 1724.2477\n","Epoch 144/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1738.8895 - val_loss: 1722.6019\n","Epoch 145/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1737.1785 - val_loss: 1720.9614\n","Epoch 146/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1735.4581 - val_loss: 1719.3149\n","Epoch 147/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1733.7356 - val_loss: 1717.6464\n","Epoch 148/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1731.9895 - val_loss: 1715.9691\n","Epoch 149/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1730.2550 - val_loss: 1714.2759\n","Epoch 150/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1728.5208 - val_loss: 1712.5609\n","Epoch 151/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1726.7571 - val_loss: 1710.8370\n","Epoch 152/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1724.9833 - val_loss: 1709.1385\n","Epoch 153/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1723.1613 - val_loss: 1707.4487\n","Epoch 154/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1721.3759 - val_loss: 1705.7031\n","Epoch 155/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1719.5383 - val_loss: 1703.9567\n","Epoch 156/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1717.7173 - val_loss: 1702.1890\n","Epoch 157/250\n","11/11 [==============================] - 0s 2ms/step - loss: 1715.8647 - val_loss: 1700.4258\n","Epoch 158/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1714.0403 - val_loss: 1698.6239\n","Epoch 159/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1712.1493 - val_loss: 1696.8335\n","Epoch 160/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1710.2894 - val_loss: 1695.0195\n","Epoch 161/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1708.4321 - val_loss: 1693.1641\n","Epoch 162/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1706.5060 - val_loss: 1691.3445\n","Epoch 163/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1704.5997 - val_loss: 1689.5422\n","Epoch 164/250\n","11/11 [==============================] - 0s 2ms/step - loss: 1702.6687 - val_loss: 1687.7368\n","Epoch 165/250\n","11/11 [==============================] - 0s 2ms/step - loss: 1700.6754 - val_loss: 1685.9354\n","Epoch 166/250\n","11/11 [==============================] - 0s 2ms/step - loss: 1698.6949 - val_loss: 1684.1144\n","Epoch 167/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1696.7101 - val_loss: 1682.2585\n","Epoch 168/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1694.6688 - val_loss: 1680.3917\n","Epoch 169/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1692.6151 - val_loss: 1678.5206\n","Epoch 170/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1690.5696 - val_loss: 1676.6056\n","Epoch 171/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1688.4777 - val_loss: 1674.6946\n","Epoch 172/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1686.4122 - val_loss: 1672.7585\n","Epoch 173/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1684.3599 - val_loss: 1670.7847\n","Epoch 174/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1682.2280 - val_loss: 1668.8202\n","Epoch 175/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1680.1167 - val_loss: 1666.8253\n","Epoch 176/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1677.9957 - val_loss: 1664.8019\n","Epoch 177/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1675.8152 - val_loss: 1662.7915\n","Epoch 178/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1673.6665 - val_loss: 1660.7343\n","Epoch 179/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1671.4727 - val_loss: 1658.6779\n","Epoch 180/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1669.2469 - val_loss: 1656.6274\n","Epoch 181/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1667.0765 - val_loss: 1654.5114\n","Epoch 182/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1664.8254 - val_loss: 1652.3794\n","Epoch 183/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1662.5657 - val_loss: 1650.2345\n","Epoch 184/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1660.2661 - val_loss: 1648.0742\n","Epoch 185/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1657.9622 - val_loss: 1645.8914\n","Epoch 186/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1655.6727 - val_loss: 1643.6793\n","Epoch 187/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1653.3328 - val_loss: 1641.5007\n","Epoch 188/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1650.9789 - val_loss: 1639.3149\n","Epoch 189/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1648.6177 - val_loss: 1637.1062\n","Epoch 190/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1646.2477 - val_loss: 1634.8629\n","Epoch 191/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1643.8094 - val_loss: 1632.6616\n","Epoch 192/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1641.4656 - val_loss: 1630.3885\n","Epoch 193/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1639.0300 - val_loss: 1628.1251\n","Epoch 194/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1636.6616 - val_loss: 1625.7766\n","Epoch 195/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1634.1735 - val_loss: 1623.4508\n","Epoch 196/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1631.6798 - val_loss: 1621.1101\n","Epoch 197/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1629.1805 - val_loss: 1618.7450\n","Epoch 198/250\n","11/11 [==============================] - 0s 2ms/step - loss: 1626.6731 - val_loss: 1616.3629\n","Epoch 199/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1624.1542 - val_loss: 1613.9513\n","Epoch 200/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1621.6188 - val_loss: 1611.5112\n","Epoch 201/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1619.0480 - val_loss: 1609.0601\n","Epoch 202/250\n","11/11 [==============================] - 0s 5ms/step - loss: 1616.4751 - val_loss: 1606.6051\n","Epoch 203/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1613.8656 - val_loss: 1604.1492\n","Epoch 204/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1611.3185 - val_loss: 1601.6254\n","Epoch 205/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1608.7106 - val_loss: 1599.0802\n","Epoch 206/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1606.0984 - val_loss: 1596.4902\n","Epoch 207/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1603.3695 - val_loss: 1593.9836\n","Epoch 208/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1600.7480 - val_loss: 1591.4159\n","Epoch 209/250\n","11/11 [==============================] - 0s 2ms/step - loss: 1598.0399 - val_loss: 1588.8380\n","Epoch 210/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1595.3071 - val_loss: 1586.2738\n","Epoch 211/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1592.5742 - val_loss: 1583.7028\n","Epoch 212/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1589.8995 - val_loss: 1581.0590\n","Epoch 213/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1587.1089 - val_loss: 1578.4442\n","Epoch 214/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1584.3970 - val_loss: 1575.7368\n","Epoch 215/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1581.5936 - val_loss: 1573.0282\n","Epoch 216/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1578.7418 - val_loss: 1570.3469\n","Epoch 217/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1575.9219 - val_loss: 1567.6179\n","Epoch 218/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1573.1230 - val_loss: 1564.8242\n","Epoch 219/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1570.2375 - val_loss: 1562.0161\n","Epoch 220/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1567.2958 - val_loss: 1559.2273\n","Epoch 221/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1564.3804 - val_loss: 1556.4052\n","Epoch 222/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1561.4309 - val_loss: 1553.5997\n","Epoch 223/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1558.4457 - val_loss: 1550.7870\n","Epoch 224/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1555.5054 - val_loss: 1547.8918\n","Epoch 225/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1552.4351 - val_loss: 1545.0488\n","Epoch 226/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1549.4530 - val_loss: 1542.1477\n","Epoch 227/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1546.4320 - val_loss: 1539.2007\n","Epoch 228/250\n","11/11 [==============================] - 0s 4ms/step - loss: 1543.3503 - val_loss: 1536.2742\n","Epoch 229/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1540.3251 - val_loss: 1533.2820\n","Epoch 230/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1537.2208 - val_loss: 1530.2765\n","Epoch 231/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1534.1179 - val_loss: 1527.2401\n","Epoch 232/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1530.9943 - val_loss: 1524.1594\n","Epoch 233/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1527.7518 - val_loss: 1521.1288\n","Epoch 234/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1524.6171 - val_loss: 1518.0208\n","Epoch 235/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1521.3885 - val_loss: 1514.9120\n","Epoch 236/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1518.2289 - val_loss: 1511.7069\n","Epoch 237/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1514.9528 - val_loss: 1508.4996\n","Epoch 238/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1511.5952 - val_loss: 1505.3624\n","Epoch 239/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1508.3867 - val_loss: 1502.1073\n","Epoch 240/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1505.0906 - val_loss: 1498.8036\n","Epoch 241/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1501.7094 - val_loss: 1495.5106\n","Epoch 242/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1498.2903 - val_loss: 1492.2280\n","Epoch 243/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1494.9187 - val_loss: 1488.9014\n","Epoch 244/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1491.5465 - val_loss: 1485.5026\n","Epoch 245/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1488.0864 - val_loss: 1482.1028\n","Epoch 246/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1484.6111 - val_loss: 1478.6835\n","Epoch 247/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1481.1812 - val_loss: 1475.1655\n","Epoch 248/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1477.5375 - val_loss: 1471.7341\n","Epoch 249/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1474.0272 - val_loss: 1468.2294\n","Epoch 250/250\n","11/11 [==============================] - 0s 3ms/step - loss: 1470.5023 - val_loss: 1464.6497\n"]},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x1df8fb31310>"]},"metadata":{},"execution_count":104}],"source":["def build_and_compile_model(norm):\n","  model = keras.Sequential([\n","      norm,\n","      layers.Dense(100, activation='relu'),\n","      layers.Dense(100, activation='relu'),\n","      layers.Dense(1)\n","  ])\n","\n","  model.compile(loss='mean_absolute_error',\n","                optimizer=tf.keras.optimizers.Adam(0.0001))\n","  return model\n","dnn_horsepower_model = build_and_compile_model(horsepower_normalizer)\n","dnn_horsepower_model.summary()\n","\n","dnn_horsepower_model.fit(\n","    train_features['num'], train_labels,\n","    validation_split=0.2,\n","    verbose=1, epochs=250)\n"]},{"cell_type":"code","execution_count":105,"metadata":{"colab":{"background_save":true},"id":"puyY9m3GvzYu"},"outputs":[{"output_type":"stream","name":"stdout","text":["550\n"]}],"source":["print(len(prime))"]},{"cell_type":"code","execution_count":106,"metadata":{"colab":{"background_save":true},"id":"XkhAyHP-wMjw"},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001DF00A2B4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001DF00A2B4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[418.22]], dtype=float32)"]},"metadata":{},"execution_count":106}],"source":["a=[1,]\n","dnn_horsepower_model.predict(a)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOgnAVDq7KfLTS9gqwPkIFO","collapsed_sections":[],"name":"prime.ipynb","version":""},"kernelspec":{"name":"python3","display_name":"Python 3.8.6 64-bit","metadata":{"interpreter":{"hash":"d1bcec944f2cef3e4af0b28b46a7da5a07c737239b68688041367af72e9a9084"}}}},"nbformat":4,"nbformat_minor":0}